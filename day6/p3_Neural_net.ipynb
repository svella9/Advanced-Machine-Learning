{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KNatarajan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 3000\n",
    "batch_size = 128\n",
    "display_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Networrk parameters\n",
    "n_hidden = 256\n",
    "num_inputs = 784 #(28*28)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph input\n",
    "X = tf.placeholder('float', [None, num_inputs])\n",
    "Y = tf.placeholder('float', [None, num_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layer weights and bias\n",
    "weights = {\n",
    "    'hidden1' : tf.Variable(tf.random_normal([num_inputs, n_hidden])),\n",
    "    'hidden2' : tf.Variable(tf.random_normal([n_hidden, n_hidden])),\n",
    "    'out' : tf.Variable(tf.random_normal([n_hidden, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bias1': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'bias2' : tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out' : tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def neural_net(x):\n",
    "    #hidden fully connected layers with 5 neurons\n",
    "    hid_layer1 = tf.add(tf.matmul(x, weights['hidden1']), biases['bias1'])\n",
    "    hid_layer2 = tf.add(tf.matmul(hid_layer1, weights['hidden2']), biases['bias2'])\n",
    "    out_layer = tf.matmul(hid_layer2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contruct Model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Minibatch loss =  2307.1638 Training accuracy 0.203125\n",
      "Step 100 Minibatch loss =  180.86801 Training accuracy 0.8515625\n",
      "Step 200 Minibatch loss =  108.31978 Training accuracy 0.890625\n",
      "Step 300 Minibatch loss =  166.74637 Training accuracy 0.828125\n",
      "Step 400 Minibatch loss =  69.31734 Training accuracy 0.921875\n",
      "Step 500 Minibatch loss =  95.03686 Training accuracy 0.828125\n",
      "Step 600 Minibatch loss =  39.701263 Training accuracy 0.9140625\n",
      "Step 700 Minibatch loss =  65.38754 Training accuracy 0.8671875\n",
      "Step 800 Minibatch loss =  72.34621 Training accuracy 0.84375\n",
      "Step 900 Minibatch loss =  43.432938 Training accuracy 0.9296875\n",
      "Step 1000 Minibatch loss =  65.23174 Training accuracy 0.875\n",
      "Step 1100 Minibatch loss =  32.45541 Training accuracy 0.921875\n",
      "Step 1200 Minibatch loss =  45.145554 Training accuracy 0.8828125\n",
      "Step 1300 Minibatch loss =  48.5691 Training accuracy 0.8671875\n",
      "Step 1400 Minibatch loss =  29.08022 Training accuracy 0.90625\n",
      "Step 1500 Minibatch loss =  45.589478 Training accuracy 0.8828125\n",
      "Step 1600 Minibatch loss =  22.593506 Training accuracy 0.90625\n",
      "Step 1700 Minibatch loss =  13.354325 Training accuracy 0.921875\n",
      "Step 1800 Minibatch loss =  13.419588 Training accuracy 0.9453125\n",
      "Step 1900 Minibatch loss =  39.51959 Training accuracy 0.890625\n",
      "Step 2000 Minibatch loss =  41.856968 Training accuracy 0.8671875\n",
      "Step 2100 Minibatch loss =  41.44766 Training accuracy 0.8828125\n",
      "Step 2200 Minibatch loss =  24.471436 Training accuracy 0.90625\n",
      "Step 2300 Minibatch loss =  13.968084 Training accuracy 0.921875\n",
      "Step 2400 Minibatch loss =  23.388205 Training accuracy 0.8515625\n",
      "Step 2500 Minibatch loss =  18.443396 Training accuracy 0.90625\n",
      "Step 2600 Minibatch loss =  30.452759 Training accuracy 0.9375\n",
      "Step 2700 Minibatch loss =  29.929544 Training accuracy 0.8984375\n",
      "Step 2800 Minibatch loss =  13.064797 Training accuracy 0.9453125\n",
      "Step 2900 Minibatch loss =  23.390577 Training accuracy 0.90625\n",
      "Step 3000 Minibatch loss =  31.599813 Training accuracy 0.875\n",
      "Optimization finished!\n",
      "Testing Accuracy\n",
      "0.8695\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_op, feed_dict = {X: batch_x, Y : batch_y})\n",
    "        if step % display_size == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict = {X: batch_x, Y: batch_y})\n",
    "            print('Step', step, \"Minibatch loss = \", loss, \"Training accuracy\", acc)\n",
    "    \n",
    "    print('Optimization finished!')\n",
    "    print('Testing Accuracy')\n",
    "    print(sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
